{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "# from torchsummary import summary\n",
    "\n",
    "# import gym\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import time\n",
    "import json\n",
    "from src.Runner2048 import Game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# So we can run off of the GPU for our tensors\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Live plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [10, 12]\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "print(device)\n",
    "\n",
    "OHE = True\n",
    "CNN = True\n",
    "XTRA_IN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Game(seed=1, board_size=4, reward_type='duration_and_largest')\n",
    "env = Game(seed=1, board_size=4, reward_type='combine_score')\n",
    "action_dict = {0:'U', 1:'R', 2:'D', 3:'L'}\n",
    "# action_dict_rev = {'U':0, 'R':1, 'D':2, 'L':3}\n",
    "\n",
    "# env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nS = 16\n",
    "nA = 4\n",
    "\n",
    "# Hyperparameters\n",
    "# BATCH_SIZE = 2**7\n",
    "BATCH_SIZE = 2**5\n",
    "\n",
    "# LAYER1_SIZE = 128\n",
    "# LAYER2_SIZE = 128\n",
    "LAYER1_SIZE = 64\n",
    "LAYER2_SIZE = 128\n",
    "\n",
    "# EPISODES_TRAINING = 1000\n",
    "# EPISODES_TRAINING = 2000\n",
    "EPISODES_TRAINING = 5000\n",
    "\n",
    "# ALPHA = 1e-5\n",
    "ALPHA = 1e-2\n",
    "GAMMA = 0.999\n",
    "# TAU = 0.005\n",
    "# TAU = 0.01\n",
    "TAU = 0.10\n",
    "# EPSILON_MAX = 1.00\n",
    "# EPSILON_MIN = 0.05\n",
    "# EPSILON_DECAY = 350\n",
    "EPSILON_MAX = 0.00\n",
    "EPSILON_MIN = 0.07\n",
    "# EPSILON_DATA = [EPSILON_MAX, EPSILON_MIN, EPSILON_DECAY]\n",
    "\n",
    "BUFFER_SIZE = 100000\n",
    "# BUFFER_SIZE = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARST = namedtuple(\"SARST\", [\"S\", \"A\", \"R\", \"S_prime\", \"T\"])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(SARST(*args))\n",
    "        # Rotate the board and add it to the buffer\n",
    "        if (False):\n",
    "            # 90 degrees CCW\n",
    "            S_new = tensor([np.rot90(args[0].cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (args[1]-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(args[3].cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 180 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 270 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # Flip Virt\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            if (args[1]%2 == 0):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "            \n",
    "            # Flip Horz\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            if (args[1]%2 == 1):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(board):\n",
    "  board_flat = torch.LongTensor(board)\n",
    "  board_flat = nn.functional.one_hot(board_flat, num_classes=16).float().flatten()\n",
    "  board_flat = board_flat.reshape(1, 4, 4, 16).permute(0, 3, 1, 2)\n",
    "  return board_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDBUFF = namedtuple(\"BOARDBUFF\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDBUFF(*args))\n",
    "\n",
    "    # def sample(self, sample_size):\n",
    "    #     return random.sample(self.buffer, sample_size)\n",
    "\n",
    "    def pop_sample(self):\n",
    "        pop_index = random.randint(0, len(self.buffer)-1)\n",
    "        return_board = self.buffer[pop_index]\n",
    "        del self.buffer[pop_index]\n",
    "        return return_board\n",
    "    \n",
    "    # def remove(self, occurence):\n",
    "    #     self.buffer.remove(occurence)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "BOARDCACHE = namedtuple(\"BOARDCACHE\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardCache(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDCACHE(*args))\n",
    "\n",
    "    def get_first(self):\n",
    "        return self.buffer.popleft()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_LAYER, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_LAYER, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "class DQN2(nn.Module):\n",
    "    def __init__(self, INPUT_LAYER, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQN2, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_LAYER+2, LAYER1_SIZE),  # +2 for the duration and max tile value\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "class DQCNN(nn.Module):\n",
    "    def __init__(self, KERNEL_SIZE, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 2 - 3 // 1 + 1 = 4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 1x4x4  -> 16x4x4\n",
    "            nn.ReLU(),  # 16x4x4\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 16x4x4 -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Flatten(),   # 32*4*4 = 512\n",
    "            nn.Linear(512, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "class HLCNN(nn.Module): # Human Level Control NN (Paper)\n",
    "    def __init__(self, OUTPUT_LAYER):\n",
    "        super(HLCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 4 - 4 // 1 + 1 = 4\n",
    "            # nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            # nn.ReLU(),  # 32x4x4\n",
    "            # nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            # nn.ReLU(),  # 64x4x4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            nn.ReLU(),  # 64x4x4\n",
    "            nn.Flatten(),   # 64*4*4 = 1024\n",
    "            nn.Linear(1024, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, OUTPUT_LAYER),  # 512\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(state, network, nA, epsilon, invalid_actions):\n",
    "    # Decide if we are going to be greedy or not\n",
    "    greedy = (random.random() > epsilon)\n",
    "\n",
    "    if greedy:\n",
    "        # Pick best action, if tie, use lowest index\n",
    "        with torch.no_grad():   # Speeds up computation\n",
    "            if (not CNN):\n",
    "                # Change for NN\n",
    "                output = network(torch.FloatTensor(state))\n",
    "                for invalid in invalid_actions:\n",
    "                    output[0, invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "            else:\n",
    "                # Change for CNN\n",
    "                output = network(state.view(1,1,4,4))\n",
    "                for invalid in invalid_actions:\n",
    "                    output[0, invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "                # return network(state.view(1,1,4,4)).argmax().item()\n",
    "\n",
    "    else:\n",
    "        # Explore\n",
    "        valid_actions = list(range(nA))\n",
    "        for invalid in invalid_actions:\n",
    "            valid_actions.remove(invalid)\n",
    "        return random.choice(valid_actions)\n",
    "        # return tensor(random.randint(0, nA-1), device=device, dtype=torch.long).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not CNN):\n",
    "    if (not XTRA_IN):\n",
    "        policy_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "        target_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "    else:\n",
    "        policy_net = DQN2(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "        target_net = DQN2(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "    # policy_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "    # target_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "else:\n",
    "    policy_net = HLCNN(nA).to(device)                                 # Change for CNN\n",
    "    target_net = HLCNN(nA).to(device)                                 # Change for CNN\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=ALPHA, amsgrad=True)\n",
    "RB = ReplayBuffer(BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = 0\n",
    "# def getEpsilon():\n",
    "#     global steps\n",
    "#     epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-steps/EPSILON_DECAY)\n",
    "#     steps += 1\n",
    "#     return epsilon\n",
    "\n",
    "max_game = 1\n",
    "def getEpsilon(game_duration):\n",
    "    global max_game\n",
    "    epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-game_duration/(max_game))\n",
    "    return epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(title, axis, args, save_string=\"\"):\n",
    "    threshold = 50\n",
    "    n_plots = len(args)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(n_plots, 1, sharex=True)\n",
    "    for argi, arg in enumerate(args):\n",
    "        data = torch.tensor(arg, dtype=torch.float)\n",
    "        # ax[argi].clf()\n",
    "        # ax[argi].clear()\n",
    "        ax[argi].set_title(title[argi])\n",
    "        ax[argi].set_ylabel(axis[argi])\n",
    "        # ax[argi].set_xlabel('Episode')\n",
    "        # ax[argi].plot(data)\n",
    "        ax[argi].step(list(range(len(data))), data)\n",
    "        ax[argi].grid()\n",
    "        ax[argi].minorticks_on()\n",
    "\n",
    "\n",
    "        # Take threshold episode averages and plot them too\n",
    "        if len(arg) >= threshold:\n",
    "            means = data.unfold(0, threshold, 1).mean(1).view(-1)\n",
    "            # means = torch.cat((torch.zeros(49), means))\n",
    "            # ax[argi].plot(means.numpy())\n",
    "            ax[argi].plot(list(range(threshold-1, len(data))), means.numpy())\n",
    "            \n",
    "    plt.xlabel('Episode')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)\n",
    "    if save_string != \"\":\n",
    "        fig.savefig(\"./figures/\"+save_string+\".png\")\n",
    "\n",
    "        # if is_ipython:\n",
    "        #     display.display(plt.gcf())\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    minibatch_awk = RB.sample(BATCH_SIZE)\n",
    "    minibatch = SARST(*zip(*minibatch_awk))\n",
    "    \n",
    "    N = len(minibatch.S)\n",
    "\n",
    "    S = torch.cat(minibatch.S).to(device)\n",
    "    if (CNN):\n",
    "        S = S.view(N, 1, 4, 4)\n",
    "    A = minibatch.A\n",
    "    torch_R = torch.cat(minibatch.R).to(device)\n",
    "    torch_maxQ = torch.zeros(N, 1).to(device)\n",
    "    nonterm_mask = tensor(minibatch.T).to(device)\n",
    "    # term_mask = tensor(list(np.array(minibatch.T)==False))\n",
    "        \n",
    "    Q_SA = policy_net(S).gather(1, torch.reshape(tensor(A).to(device), [N, 1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if (not CNN):\n",
    "            # Change for NN\n",
    "            S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "            torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "        else:\n",
    "            # Change for CNN\n",
    "            S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "            torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked).to(device).view(sum(nonterm_mask), 1, 4, 4)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "\n",
    "\n",
    "    y = (torch_maxQ * GAMMA) + torch_R\n",
    "\n",
    "    # Compute Huber loss\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    # loss = criterion(Q_SA, y)\n",
    "\n",
    "    # # MSE\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(Q_SA, y)\n",
    "    # MSE Clipped\n",
    "    # criterion = nn.MSELoss()\n",
    "    # loss = criterion(Q_SA, y)\n",
    "    # loss = loss.clamp(min=-1, max=1)\n",
    "\n",
    "    # Clipping the loss between -1 and 1\n",
    "    # loss = torch.mean(torch.maximum(torch.minimum((Q_SA-y)**2, tensor(1)), tensor(-1)))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = []\n",
    "episodic_loss = []\n",
    "episodic_epsilon = []\n",
    "episdoic_duration = []\n",
    "episdoic_max_tile = []\n",
    "episodic_invalid_moves_made_count = []\n",
    "\n",
    "# use_midgame_board = False\n",
    "# BB = BoardBuffer(100)\n",
    "# BC = BoardCache(50)\n",
    "\n",
    "def DQN_network(episodes):\n",
    "    global max_game\n",
    "    save_tag = \"combine_score_good_moves_only_NN_dynamic_epsilon_board_normalize_lr-2_gam999_tau10_clipped_loss\"\n",
    "    start_time = time.time()\n",
    "    T = 0\n",
    "    for epi in range(episodes):\n",
    "        \n",
    "        # BC.clear()\n",
    "        # if (use_midgame_board and (len(BB) > 100) and (0.1 > random.random())): # Randomly select from the board buffer\n",
    "        #     loadBB = BB.pop_sample()\n",
    "        #     S = env.load_board(loadBB.S, loadBB.D)\n",
    "        #     S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # else:\n",
    "        if (not XTRA_IN):\n",
    "            S = env.reset()   \n",
    "        else:\n",
    "            S = np.append(env.reset(), [env.game_duration, env.largest_value])\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "            # BC.push(S, 0)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_mean_loss = 0\n",
    "        terminated = False\n",
    "        episodic_invalid_moves_made = 0\n",
    "        \n",
    "        # epsilon = getEpsilon()\n",
    "        \n",
    "        mini_duration = 0\n",
    "\n",
    "        invalid_moves = []\n",
    "        while not terminated:\n",
    "            T += 1\n",
    "            mini_duration += 1\n",
    "            # if True and epi%10==0:\n",
    "            #     env.render()\n",
    "\n",
    "            epsilon = getEpsilon(env.game_duration)\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, epsilon, invalid_moves)\n",
    "            # Take step\n",
    "            (reward, terminated, updated, invalid_moves, invalid_moves_made) = env.swipe(action_dict[A])\n",
    "            if (not XTRA_IN):\n",
    "                S_prime = env.get_flat_board()\n",
    "            else:\n",
    "                S_prime = np.append(env.get_flat_board(), [env.game_duration, env.largest_value])\n",
    "            episodic_invalid_moves_made += invalid_moves_made\n",
    "            \n",
    "            # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "            S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition\n",
    "            # RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "            #         S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "            RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                    S_prime, not terminated)\n",
    "\n",
    "            S = S_prime\n",
    "            # if (terminated):\n",
    "            #     BC.push(S, env.game_duration) # Just filler really \n",
    "            # else:\n",
    "            #     BC.push(S.view(4,4).cpu().numpy(), env.game_duration)\n",
    "            \n",
    "            # Update the networks networks\n",
    "            if len(RB) > BATCH_SIZE:\n",
    "                episodic_mean_loss += train()\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            if T%100==0:\n",
    "                # Soft update of the target network's weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        # if (mini_duration > 50):\n",
    "        #     tempBC = BC.get_first()\n",
    "        #     BB.push(tempBC[0], tempBC[1])\n",
    "\n",
    "        episodic_epsilon.append(epsilon)\n",
    "        episodic_loss.append(episodic_mean_loss/T)\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episdoic_duration.append(env.game_duration)\n",
    "        episdoic_max_tile.append(np.log2(env.largest_value))\n",
    "        episodic_invalid_moves_made_count.append(episodic_invalid_moves_made)\n",
    "\n",
    "        max_game = max(max_game, env.game_duration)\n",
    "        if epi % 100 == 0:\n",
    "            save_string = \"_policy_weights_episode_\"+str(epi).zfill(4)\n",
    "            torch.save(target_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"target\"+save_string+\".pth\")\n",
    "            torch.save(policy_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"policy\"+save_string+\".pth\")\n",
    "                \n",
    "            # # Soft update of the target network's weights (do at end of episode)\n",
    "            # target_net_state_dict = target_net.state_dict()\n",
    "            # policy_net_state_dict = policy_net.state_dict()\n",
    "            # for key in policy_net_state_dict:\n",
    "            #     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            # target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            \n",
    "        # if epi % 50 == 0:\n",
    "        #     print(epsilon)\n",
    "        #     env.display()\n",
    "        #     plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Episodic Value...\", \"Invalid Moves...\", \"Epsilon Value...\"], \n",
    "        #                 [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Invalid Moves\", \"Epsilon\"], \n",
    "        #                 [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_revision_count, episodic_epsilon])\n",
    "        \n",
    "        if epi % 10 == 0:\n",
    "            print(epi)\n",
    "            env.display()\n",
    "            plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Tile Value...\", \"Epsilon Value...\"], \n",
    "                        [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "                        [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_epsilon])\n",
    "        # plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Episodic Value...\", \"Invalid Moves...\", \"Epsilon Value...\"], \n",
    "        #             [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Invalid Moves\", \"Epsilon\"], \n",
    "        #             [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_revision_count, episodic_epsilon])\n",
    "    \n",
    "                \n",
    "    \n",
    "    delta_time = time.time()-start_time\n",
    "    # plot_multi([\"Reward History\", \"Loss History\", \"Duration\", \"Max Episodic Value\", \"Invalid Moves\", \"Epsilon History\"], \n",
    "    #            [\"Reward\", \"Loss\", \"Duration\", \"Max Tile Value\", \"Invalid Moves\", \"Epsilon\"], \n",
    "    #            [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_revision_count, episodic_epsilon], save_string=\"model_\"+save_tag+\"\")\n",
    "    plot_multi([\"Reward History\", \"Loss History\", \"Duration\", \"Max Tile History\", \"Epsilon History\"], \n",
    "               [\"Reward\", \"Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "               [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_epsilon], save_string=\"model_\"+save_tag+\"\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save data\n",
    "    # data_file = open(\"./trainged_models/data.json\", 'w+')\n",
    "    # json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "    #                 \"episodic_loss\": episodic_loss, \n",
    "    #                 \"episodic_epsilon\": episodic_epsilon,\n",
    "    #                 \"training_time\": delta_time\n",
    "    #                 }\n",
    "    # json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDQN_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES_TRAINING\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# env.close()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 79\u001b[0m, in \u001b[0;36mDQN_network\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# if (terminated):\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#     BC.push(S, env.game_duration) # Just filler really \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     BC.push(S.view(4,4).cpu().numpy(), env.game_duration)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Update the networks networks\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(RB) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[0;32m---> 79\u001b[0m     episodic_mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_2048/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_2048/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_2048/lib/python3.12/site-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_2048/lib/python3.12/site-packages/torch/optim/adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 339\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_2048/lib/python3.12/site-packages/torch/optim/adamw.py:419\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    418\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 419\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    422\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DQN_network(EPISODES_TRAINING)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = open(\"./trainged_models/data_CNN.json\", 'w+')\n",
    "# json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "#                 \"episodic_loss\": episodic_loss, \n",
    "#                 \"episodic_epsilon\": episodic_epsilon,\n",
    "#                 \"training_time\": delta_time\n",
    "#                 }\n",
    "# json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "def test(episodes):\n",
    "    episodic_rewards = []\n",
    "    episodic_durations = []\n",
    "\n",
    "    for epi in range(episodes):\n",
    "\n",
    "        \n",
    "        S = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_duration = 0\n",
    "        T = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            episodic_duration += 1\n",
    "            env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, -1)\n",
    "            # Take step\n",
    "            S_prime, reward, terminated, _ = env.step(A)\n",
    "            S = S_prime\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episodic_durations.append(episodic_duration)\n",
    "        # if epi % 10 == 0:\n",
    "        print(epi, \"of\", episodes)\n",
    "        plot_multi([\"Training Rewards...\", \"Training Durations...\"], \n",
    "                    [\"Reward\", \"Episode Duration\"], \n",
    "                    [episodic_rewards, episodic_durations])\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_FUCK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
