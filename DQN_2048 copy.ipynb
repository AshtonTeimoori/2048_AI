{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "# from torchsummary import summary\n",
    "\n",
    "# import gym\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import time\n",
    "import json\n",
    "from src.Runner2048 import Game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# So we can run off of the GPU for our tensors\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Live plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [10, 12]\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "print(device)\n",
    "\n",
    "OHE = True\n",
    "CNN = True\n",
    "XTRA_IN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game(seed=None, board_size=4, reward_type='hs')\n",
    "action_dict = {0:'U', 1:'R', 2:'D', 3:'L'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nS = 16\n",
    "nA = 4\n",
    "\n",
    "# Hyperparameters\n",
    "# BATCH_SIZE = 2**7\n",
    "# BATCH_SIZE = 2**5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# LAYER1_SIZE = 128\n",
    "# LAYER2_SIZE = 128\n",
    "# LAYER1_SIZE = 64\n",
    "# LAYER2_SIZE = 128\n",
    "\n",
    "# EPISODES_TRAINING = 1000\n",
    "# EPISODES_TRAINING = 2000\n",
    "EPISODES_TRAINING = 25000\n",
    "\n",
    "ALPHA = 1e-3\n",
    "# ALPHA = 1e-2\n",
    "GAMMA = 0.99\n",
    "# TAU = 0.005\n",
    "TAU = 0.005\n",
    "# TAU = 1.00\n",
    "# EPSILON_MAX = 1.00\n",
    "# EPSILON_MIN = 0.05\n",
    "# EPSILON_DECAY = 350\n",
    "EPSILON_MAX = 0.00\n",
    "# EPSILON_MIN = 0.07\n",
    "EPSILON_MIN = 0.05\n",
    "# EPSILON_DATA = [EPSILON_MAX, EPSILON_MIN, EPSILON_DECAY]\n",
    "\n",
    "BUFFER_SIZE = 50000\n",
    "# BUFFER_SIZE = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARST = namedtuple(\"SARST\", [\"S\", \"A\", \"R\", \"S_prime\", \"T\"])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "        self.probs = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(SARST(*args))\n",
    "    # def push(self, *args):\n",
    "    #     self.buffer.append(SARST(*args))\n",
    "    #     max_val = 0\n",
    "    #     if args[3] == [0]:\n",
    "    #         for i in range(0,15):\n",
    "    #             if torch.sum(args[0].reshape(16,4,4)[i,:,:]) > 0:\n",
    "    #                 max_val = i+1\n",
    "    #     else:\n",
    "    #         for i in range(0,15):\n",
    "    #             if torch.sum(args[3].reshape(16,4,4)[i,:,:]) > 0:\n",
    "    #                 max_val = i+1\n",
    "    #     # self.probs.append(max(args[3]).cpu().item())\n",
    "    #     self.probs.append(2*max_val)\n",
    "    #     # # Rotate the board and add it to the buffer\n",
    "    #     # if (False):\n",
    "    #     #     # 90 degrees CCW\n",
    "    #     #     S_new = tensor([np.rot90(args[0].cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     A_new = (args[1]-1)%4\n",
    "    #     #     if (args[3] == [0]):\n",
    "    #     #         S_prime_new = args[3]\n",
    "    #     #     else:\n",
    "    #     #         S_prime_new = tensor([np.rot90(args[3].cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "    #     #     # 180 degrees CCW\n",
    "    #     #     S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     A_new = (A_new-1)%4\n",
    "    #     #     if (args[3] == [0]):\n",
    "    #     #         S_prime_new = args[3]\n",
    "    #     #     else:\n",
    "    #     #         S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "    #     #     # 270 degrees CCW\n",
    "    #     #     S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     A_new = (A_new-1)%4\n",
    "    #     #     if (args[3] == [0]):\n",
    "    #     #         S_prime_new = args[3]\n",
    "    #     #     else:\n",
    "    #     #         S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "    #     #     self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "    #     #     # Flip Virt\n",
    "    #     #     S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "    #     #     if (args[1]%2 == 0):\n",
    "    #     #         A_new = (args[1]-2)%4\n",
    "    #     #     else:\n",
    "    #     #         A_new = args[1]\n",
    "            \n",
    "    #     #     if (args[3] == [0]):\n",
    "    #     #         S_prime_new = args[3]\n",
    "    #     #     else:\n",
    "    #     #         S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "    #     #     self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "            \n",
    "    #     #     # Flip Horz\n",
    "    #     #     S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "    #     #     if (args[1]%2 == 1):\n",
    "    #     #         A_new = (args[1]-2)%4\n",
    "    #     #     else:\n",
    "    #     #         A_new = args[1]\n",
    "            \n",
    "    #     #     if (args[3] == [0]):\n",
    "    #     #         S_prime_new = args[3]\n",
    "    #     #     else:\n",
    "    #     #         S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "    #     #     self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "        # return random.choices(self.buffer, weights=np.array(self.probs), k=sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(board):\n",
    "  board_flat = torch.LongTensor(board)\n",
    "  board_flat = nn.functional.one_hot(board_flat, num_classes=16).float().flatten()\n",
    "  board_flat = board_flat.reshape(1, 4, 4, 16).permute(0, 3, 1, 2)\n",
    "  return board_flat\n",
    "\n",
    "# # Sort in descending order, so the largest OHE is at the top and the lower are descending \n",
    "# def encode_state(board):\n",
    "#   board_flat = torch.LongTensor(board)\n",
    "#   board_flat = nn.functional.one_hot(board_flat, num_classes=16).float().flatten()\n",
    "#   board_flat = board_flat.reshape(1, 4, 4, 16).permute(0, 3, 1, 2).flip(dims=[1])\n",
    "#   zeros = board_flat[0, 15, :, :]\n",
    "#   maxi = np.max(board)\n",
    "  \n",
    "#   i=0\n",
    "#   while i < maxi:\n",
    "#     temp_board = board_flat[0, i, :, :].detach().clone()\n",
    "#     board_flat[0, i, :, :] = board_flat[0, 15-maxi+i, :, :].detach().clone()\n",
    "#     board_flat[0, 15-maxi+i, :, :] = temp_board\n",
    "#     i += 1\n",
    "#   return board_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDBUFF = namedtuple(\"BOARDBUFF\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDBUFF(*args))\n",
    "\n",
    "    def pop_sample(self):\n",
    "        pop_index = random.randint(0, len(self.buffer)-1)\n",
    "        return_board = self.buffer[pop_index]\n",
    "        del self.buffer[pop_index]\n",
    "        return return_board\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "BOARDCACHE = namedtuple(\"BOARDCACHE\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardCache(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDCACHE(*args))\n",
    "\n",
    "    def get_first(self):\n",
    "        return self.buffer.popleft()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features that we care about only tend to be in the verical or horizontal directions, we would only want to find correlation between those. Looking in the diagonals would cause extra noise. Since we cannot move in diagonals, it would be efficient to look only vertically and horizontally. It would stand to reason that a square kernel could also learn these features, however, it would require more training and the values at the corner of the kernel would be close to, if not, zero. To achieve only looking at the vertical and horizontal moves, we used non-square kernels (shape [1,2] and [2,1]) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv_vert = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, hidden_channels, kernel_size=(1,2), stride=1),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "#         self.conv_horz = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, hidden_channels, kernel_size=(2,1), stride=1),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "#         self.conv_vert2 = nn.Sequential(\n",
    "#             nn.Conv2d(hidden_channels, out_channels, kernel_size=(1,2), stride=1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "#         self.conv_horz2 = nn.Sequential(\n",
    "#             nn.Conv2d(hidden_channels, out_channels, kernel_size=(2,1), stride=1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x_vert = self.conv_vert(x)\n",
    "#         x_horz = self.conv_horz(x)\n",
    "#         x_vert2vert = self.conv_vert2(x_vert)\n",
    "#         x_vert2horz = self.conv_horz2(x_vert)\n",
    "#         x_horz2vert = self.conv_vert2(x_horz)\n",
    "#         x_horz2horz = self.conv_horz2(x_horz)\n",
    "#         return torch.cat([x_vert2vert, x_vert2horz, x_horz2vert, x_horz2horz], dim=1)\n",
    "\n",
    "# class NONSQUARE(nn.Module): # \n",
    "#     def __init__(self, HIDDEN_LAYER_1, HIDDEN_LAYER_2, OUTPUT_LAYER):\n",
    "#         super(NONSQUARE, self).__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             ConvBlock(16, 256, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),   # Unnecessary?\n",
    "#             nn.Linear(17408, HIDDEN_LAYER_1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN_LAYER_1, HIDDEN_LAYER_2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN_LAYER_2, OUTPUT_LAYER)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv_2x2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=2, stride=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv_4x4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=1),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv_2x2_w_2x2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=2, stride=1),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv_3x3_w_2x2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=2, stride=1),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_1x1xhid = self.conv_4x4(x)\n",
    "        hid_2x2xhid = self.conv_3x3(x)\n",
    "        hid_3x3xhid = self.conv_2x2(x)\n",
    "        out_1x1xout = self.conv_2x2_w_2x2(hid_2x2xhid)\n",
    "        out_2x2xout = self.conv_3x3_w_2x2(hid_3x3xhid)\n",
    "        return torch.cat([out_1x1xhid, self.flatten(hid_2x2xhid), out_1x1xout, out_2x2xout], dim=1)\n",
    "\n",
    "class CNN234(nn.Module): # \n",
    "    def __init__(self, HIDDEN_LAYER_1, HIDDEN_LAYER_2, OUTPUT_LAYER):\n",
    "        super(CNN234, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            ConvBlock(16, 1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),   # Unnecessary?\n",
    "            nn.Linear(15360, HIDDEN_LAYER_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_1, HIDDEN_LAYER_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_2, OUTPUT_LAYER)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Player\n",
    "    Reference: \n",
    "    Stuart Russell, Peter Norvig - Artificial Intelligence A Modern Approach - Pearson Higher Education (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility will be the evaluation of the policy network. When it is the agents move, the utility value will simply be the q-value of the action taken. When the computer places a piece on the board, the utility value will be the argmax of the new board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Inputs\n",
    "#  The game\n",
    "#  The policy that was used to make the move\n",
    "#  The Q-value from the action to get to this state\n",
    "def minimax(game, policy, value_from_action, depth=3, my_turn=True):\n",
    "\n",
    "    best_action = None\n",
    "\n",
    "\n",
    "    # See if the game is over or we've reached the depth limit\n",
    "    if depth <= 1 or game.is_game_over():\n",
    "        val = value_from_action\n",
    "        return (best_action, val)\n",
    "\n",
    "    if my_turn:\n",
    "        # Want to maximize \n",
    "        val = float('-inf')\n",
    "        actions = game.get_possible_moves()\n",
    "        if len(actions) == 1:\n",
    "            best_action = actions[0]\n",
    "\n",
    "        for A in actions:\n",
    "            # Make the move\n",
    "            temp_game = copy.deepcopy(game)\n",
    "            \n",
    "            (reward, terminated, updated, invalid_moves, invalid_moves_made) = temp_game.swipe(action_dict[A], adversarial=False)\n",
    "            value_from_action = max(policy(encode_state(temp_game.get_flat_board()))).item()\n",
    "            (tempBestMove, tempVal) = minimax(temp_game, policy, value_from_action, depth-1, not my_turn)\n",
    "    else:\n",
    "        # Want to minimize\n",
    "        val = float('inf')\n",
    "        actions = game.get_avaliable_spaces()\n",
    "        for tile_val in [2, 4]:\n",
    "            for A in actions:\n",
    "                # Make the move\n",
    "                row = int(A / 4)\n",
    "                col = A % 4\n",
    "                temp_game = copy.deepcopy(game)\n",
    "                temp_game.add_tile_to_board(row, col, tile_val)\n",
    "                # (tempBestMove, tempVal) = minimax(temp_game, policy, value_from_action, depth-1, not my_turn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return best_action, val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(state, network, nA, epsilon, invalid_actions):\n",
    "    # Decide if we are going to be greedy or not\n",
    "    greedy = (random.random() > epsilon)\n",
    "\n",
    "    if greedy:\n",
    "        # Pick best action, if tie, use lowest index\n",
    "        with torch.no_grad():   # Speeds up computation\n",
    "            if (not CNN):\n",
    "                # Change for NN\n",
    "                output = network(torch.FloatTensor(state))\n",
    "                for invalid in invalid_actions:\n",
    "                    output[0, invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "            else:\n",
    "                # Change for CNN\n",
    "                output = network(state.view(1,16,4,4))\n",
    "                # output = network(state.view(1,1,4,4))\n",
    "                for invalid in invalid_actions:\n",
    "                    output[0, invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "                # return network(state.view(1,1,4,4)).argmax().item()\n",
    "\n",
    "    else:\n",
    "        # Explore\n",
    "        valid_actions = list(range(nA))\n",
    "        for invalid in invalid_actions:\n",
    "            valid_actions.remove(invalid)\n",
    "        return random.choice(valid_actions)\n",
    "        # return tensor(random.randint(0, nA-1), device=device, dtype=torch.long).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not CNN):\n",
    "    if (not XTRA_IN):\n",
    "        policy_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "        target_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "    else:\n",
    "        policy_net = DQN2(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "        target_net = DQN2(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)       # Change for NN\n",
    "    # policy_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "    # target_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "else:\n",
    "    # policy_net = HLCNN(nA).to(device)                                 # Change for CNN\n",
    "    # target_net = HLCNN(nA).to(device)                                 # Change for CNN\n",
    "    # policy_net = DQCNN16(1024, 4).to(device)                                 # Change for CNN\n",
    "    # target_net = DQCNN16(1024, 4).to(device)                                 # Change for CNN\n",
    "    # policy_net = NONSQUARE(2048, 256, 4).to(device)                                 # Change for NONSQUARE\n",
    "    # target_net = NONSQUARE(2048, 256, 4).to(device)                                 # Change for NONSQUARE\n",
    "    policy_net = CNN234(1024, 256, 4).to(device)                                 # Change for CNN234\n",
    "    target_net = CNN234(1024, 256, 4).to(device)                                 # Change for CNN234                              # Change for CNN234\n",
    "\n",
    "load_model = False\n",
    "if load_model:\n",
    "    model_path = 'backup_trained_models\\cnn_256_consistent.pth'\n",
    "    target_net.load_state_dict(torch.load(model_path))\n",
    "    policy_net.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=ALPHA, amsgrad=True)\n",
    "RB = ReplayBuffer(BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = 0\n",
    "# def getEpsilon():\n",
    "#     global steps\n",
    "#     epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-steps/EPSILON_DECAY)\n",
    "#     steps += 1\n",
    "#     return epsilon\n",
    "\n",
    "# max_game = 0\n",
    "max_game = 1\n",
    "def getEpsilon(game_duration):\n",
    "    global max_game\n",
    "    if (max_game == 0):\n",
    "        return 1\n",
    "    epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-game_duration/(max_game))\n",
    "    return epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(title, axis, args, save_string=\"\"):\n",
    "    threshold = 50\n",
    "    n_plots = len(args)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(n_plots, 1, sharex=True)\n",
    "    for argi, arg in enumerate(args):\n",
    "        data = torch.tensor(arg, dtype=torch.float)\n",
    "        if len(data) > threshold:\n",
    "            ax[argi].set_title(title[argi])\n",
    "            ax[argi].set_ylabel(axis[argi])\n",
    "            ax[argi].step(list(range(len(data)))[threshold:], data[threshold:])\n",
    "            ax[argi].grid()\n",
    "            ax[argi].minorticks_on()\n",
    "\n",
    "\n",
    "            # Take threshold episode averages and plot them too\n",
    "            if len(arg) >= threshold:\n",
    "                means = data.unfold(0, threshold, 1).mean(1).view(-1)\n",
    "                ax[argi].plot(list(range(threshold, len(data))), means.numpy()[1:])\n",
    "            \n",
    "    plt.xlabel('Episode')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)\n",
    "    if save_string != \"\":\n",
    "        fig.savefig(\"./figures/\"+save_string+\".png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    minibatch_awk = RB.sample(BATCH_SIZE)\n",
    "    minibatch = SARST(*zip(*minibatch_awk))\n",
    "    \n",
    "    N = len(minibatch.S)\n",
    "\n",
    "    S = torch.cat(minibatch.S).to(device)\n",
    "    if (CNN):\n",
    "        # S = S.view(N, 1, 4, 4)\n",
    "        S = S.view(N, 16, 4, 4)\n",
    "    A = minibatch.A\n",
    "    torch_R = torch.cat(minibatch.R).to(device)\n",
    "    torch_maxQ = torch.zeros(N, 1).to(device)\n",
    "    nonterm_mask = tensor(minibatch.T).to(device)\n",
    "    # term_mask = tensor(list(np.array(minibatch.T)==False))\n",
    "        \n",
    "    Q_SA = policy_net(S).gather(1, torch.reshape(tensor(A).to(device), [N, 1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if (not CNN):\n",
    "            # Change for NN\n",
    "            S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "            torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "        else:\n",
    "            # Change for CNN\n",
    "            S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "            # torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked).to(device).view(sum(nonterm_mask), 1, 4, 4)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "            torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked).to(device).view(sum(nonterm_mask), 16, 4, 4)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "\n",
    "\n",
    "    y = (torch_maxQ * GAMMA) + torch_R\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(Q_SA, y)\n",
    "\n",
    "    # # MSE\n",
    "    # criterion = nn.MSELoss()\n",
    "    # loss = criterion(Q_SA, y)\n",
    "    # loss = loss.clamp(min=-1, max=1)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = []\n",
    "episodic_loss = []\n",
    "episodic_epsilon = []\n",
    "episodic_duration = []\n",
    "episdoic_max_tile = []\n",
    "episodic_invalid_moves_made_count = []\n",
    "\n",
    "def DQN_network(episodes):\n",
    "    \n",
    "    # burn in to collect batch data\n",
    "    for i in range(1000):\n",
    "        if (not XTRA_IN):\n",
    "            S = env.reset()   \n",
    "        else:\n",
    "            S = np.append(env.reset(), [env.game_duration, env.largest_value])\n",
    "        \n",
    "        if (OHE):\n",
    "            S = encode_state(S).flatten()\n",
    "            S = torch.tensor(S, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "            \n",
    "            while not terminated:\n",
    "                valid_actions = env.get_possible_moves()\n",
    "                A = random.randchoice(valid_actions)\n",
    "                (reward, terminated, updated, invalid_moves, invalid_moves_made) = env.swipe(action_dict[A])\n",
    "                if (not XTRA_IN):\n",
    "                    S_prime = env.get_flat_board()\n",
    "                else:\n",
    "                    S_prime = np.append(env.get_flat_board(), [env.game_duration, env.largest_value])\n",
    "                    \n",
    "                if (OHE):\n",
    "                    S_prime = encode_state(S_prime).flatten()\n",
    "                    S_prime = [0] if terminated else tensor(S_prime, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "                    S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "                # Store the transition\n",
    "                # RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                #         S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "                RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                        S_prime, not terminated)\n",
    "\n",
    "                S = S_prime\n",
    "    \n",
    "    \n",
    "    global max_game\n",
    "    save_tag = \"white_mono_corner_CNN234\"\n",
    "    start_time = time.time()\n",
    "    T = 0\n",
    "    for epi in range(episodes):\n",
    "\n",
    "        if (not XTRA_IN):\n",
    "            S = env.reset()   \n",
    "        else:\n",
    "            S = np.append(env.reset(), [env.game_duration, env.largest_value])\n",
    "        \n",
    "        if (OHE):\n",
    "            S = encode_state(S).flatten()\n",
    "            S = torch.tensor(S, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_mean_loss = 0\n",
    "        terminated = False\n",
    "        episodic_invalid_moves_made = 0\n",
    "        \n",
    "        mini_duration = 0\n",
    "\n",
    "        invalid_moves = []\n",
    "        reward_vect = []\n",
    "        game_states = []\n",
    "        action_vect = []\n",
    "        while not terminated:\n",
    "            T += 1\n",
    "            mini_duration += 1\n",
    "\n",
    "            epsilon = getEpsilon(env.game_duration)\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, epsilon, invalid_moves)\n",
    "            # Take step\n",
    "            (reward, terminated, updated, invalid_moves, invalid_moves_made) = env.swipe(action_dict[A])\n",
    "            if (not XTRA_IN):\n",
    "                S_prime = env.get_flat_board()\n",
    "            else:\n",
    "                S_prime = np.append(env.get_flat_board(), [env.game_duration, env.largest_value])\n",
    "            \n",
    "            episodic_invalid_moves_made += invalid_moves_made\n",
    "\n",
    "            if (OHE):\n",
    "                S_prime = encode_state(S_prime).flatten()\n",
    "                S_prime = [0] if terminated else tensor(S_prime, dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "                S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition\n",
    "            # RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "            #         S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "            RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                    S_prime, not terminated)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            # Update the networks networks\n",
    "            if len(RB) > BATCH_SIZE:\n",
    "                episodic_mean_loss += train()\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            if T%20==0:\n",
    "                # Soft update of the target network's weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "            game_states.append(np.copy(env.board))\n",
    "            action_vect.append(action_dict[A])\n",
    "            reward_vect.append(env.reward_vect)\n",
    "\n",
    "        episodic_epsilon.append(epsilon)\n",
    "        episodic_loss.append(episodic_mean_loss/T)\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episodic_duration.append(env.game_duration)\n",
    "        episdoic_max_tile.append(np.log2(env.largest_value))\n",
    "        episodic_invalid_moves_made_count.append(episodic_invalid_moves_made)\n",
    "\n",
    "        # if len(episodic_duration) < 10:\n",
    "        #     max_game = 0\n",
    "        if len(episodic_duration) < 50:\n",
    "            max_game = max(max_game, env.game_duration)\n",
    "        else:\n",
    "            max_game = max(episodic_duration[-50:])\n",
    "        if epi % 100 == 0:\n",
    "            save_string = \"_policy_weights_episode_\"+str(epi).zfill(4)\n",
    "            torch.save(target_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"target\"+save_string+\".pth\")\n",
    "            torch.save(policy_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"policy\"+save_string+\".pth\")\n",
    "            plot_multi([\"Reward History\", \"Loss History\", \"Duration\", \"Max Tile History\", \"Epsilon History\"], \n",
    "                       [\"Reward\", \"Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "                       [episodic_rewards, episodic_loss, episodic_duration, episdoic_max_tile, episodic_epsilon], save_string=\"model_\"+save_tag+\"\")\n",
    "\n",
    "        if epi % 5 == 0:\n",
    "            print(epi)\n",
    "            env.display()\n",
    "            plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Tile Value...\", \"Epsilon Value...\"], \n",
    "                        [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "                        [episodic_rewards, episodic_loss, episodic_duration, episdoic_max_tile, episodic_epsilon])\n",
    "            \n",
    "        if epi % 25 == 0:\n",
    "            with open('rewards.txt', 'w') as f:\n",
    "                for i, state in enumerate(game_states):\n",
    "                    f.write('State:\\n {}\\n'.format(state))\n",
    "                    f.write('Action: {}\\n'.format(action_vect[i]))\n",
    "                    f.write('Reward: {}\\n'.format(reward_vect[i]))\n",
    "                    f.write('\\n')\n",
    "                \n",
    "    \n",
    "    delta_time = time.time()-start_time\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save data\n",
    "    # data_file = open(\"./trainged_models/data.json\", 'w+')\n",
    "    # json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "    #                 \"episodic_loss\": episodic_loss, \n",
    "    #                 \"episodic_epsilon\": episodic_epsilon,\n",
    "    #                 \"training_time\": delta_time\n",
    "    #                 }\n",
    "    # json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDQN_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES_TRAINING\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 109\u001b[0m, in \u001b[0;36mDQN_network\u001b[1;34m(episodes)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Update the networks networks\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(RB) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m--> 109\u001b[0m     episodic_mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DQN_network(EPISODES_TRAINING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_FUCK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
