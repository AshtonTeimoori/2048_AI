{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "# from torchsummary import summary\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import time\n",
    "import json\n",
    "from src.Runner2048 import Game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# So we can run off of the GPU for our tensors\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Live plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "print(device)\n",
    "\n",
    "OHE = True\n",
    "CNN = False\n",
    "XTRA_IN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Game(seed=1, board_size=4, reward_type='duration_and_largest')\n",
    "env = Game(seed=1, board_size=4, reward_type='hs')\n",
    "action_dict = {0:'U', 1:'R', 2:'D', 3:'L'}\n",
    "\n",
    "# env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nS = 256\n",
    "nA = 4\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "# BATCH_SIZE = 2**5\n",
    "\n",
    "LAYER1_SIZE = 1024\n",
    "LAYER2_SIZE = 1024\n",
    "\n",
    "EPISODES_TRAINING = 10000\n",
    "# EPISODES_TRAINING = 2000\n",
    "# EPISODES_TRAINING = 50000\n",
    "\n",
    "ALPHA = 0.005\n",
    "GAMMA = 0.99\n",
    "# TAU = 0.005\n",
    "TAU = 0.005\n",
    "EPSILON_MAX = 0.1\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 1000\n",
    "# EPSILON_DATA = [EPSILON_MAX, EPSILON_MIN, EPSILON_DECAY]\n",
    "\n",
    "# BUFFER_SIZE = 10000\n",
    "BUFFER_SIZE = 15000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARST = namedtuple(\"SARST\", [\"S\", \"A\", \"R\", \"S_prime\", \"T\"])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(SARST(*args))\n",
    "        # Rotate the board and add it to the buffer\n",
    "        if (False):\n",
    "            # 90 degrees CCW\n",
    "            S_new = tensor([np.rot90(args[0].cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (args[1]-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(args[3].cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 180 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 270 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # Flip Virt\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            if (args[1]%2 == 0):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "            \n",
    "            # Flip Horz\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            if (args[1]%2 == 1):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(board):\n",
    "  board_flat = torch.LongTensor(board)\n",
    "  board_flat = nn.functional.one_hot(board_flat, num_classes=16).float().flatten()\n",
    "  board_flat = board_flat.reshape(1, 4, 4, 16).permute(0, 3, 1, 2)\n",
    "  return board_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDBUFF = namedtuple(\"BOARDBUFF\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDBUFF(*args))\n",
    "\n",
    "    # def sample(self, sample_size):\n",
    "    #     return random.sample(self.buffer, sample_size)\n",
    "\n",
    "    def pop_sample(self):\n",
    "        pop_index = random.randint(0, len(self.buffer)-1)\n",
    "        return_board = self.buffer[pop_index]\n",
    "        del self.buffer[pop_index]\n",
    "        return return_board\n",
    "    \n",
    "    # def remove(self, occurence):\n",
    "    #     self.buffer.remove(occurence)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "BOARDCACHE = namedtuple(\"BOARDCACHE\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardCache(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDCACHE(*args))\n",
    "\n",
    "    def get_first(self):\n",
    "        return self.buffer.popleft()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_LAYER, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_LAYER, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "    \n",
    "class DQNM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        # self.fc4 = nn.Linear(2048, 512)\n",
    "        # self.fc4b = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # x = self.leaky_relu(self.fc3(x))\n",
    "        # x = self.fc4(x)\n",
    "        # x = torch.relu(self.fc4b(x))\n",
    "        # x = torch.relu(self.fc5(x))\n",
    "        # return self.sm(self.fc3(x))\n",
    "        return self.fc5(x)\n",
    "    \n",
    "class DQCNN(nn.Module):\n",
    "    def __init__(self, KERNEL_SIZE, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 2 - 3 // 1 + 1 = 4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 1x4x4  -> 16x4x4\n",
    "            nn.ReLU(),  # 16x4x4\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 16x4x4 -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Flatten(),   # 32*4*4 = 512\n",
    "            nn.Linear(512, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "class DQCNNS(nn.Module):\n",
    "    def __init__(self, KERNEL_SIZE, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQCNNS, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 2 - 3 // 1 + 1 = 4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=64, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 1x4x4  -> 16x4x4\n",
    "            nn.ReLU(),  # 16x4x4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 16x4x4 -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=2, stride=1, padding=0),  # 32x4x4 -> 64x3x3\n",
    "            nn.Flatten(),   # 32*4*4 = 512\n",
    "            nn.Linear(2304, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "class HLCNN(nn.Module): # Human Level Control NN (Paper)\n",
    "    def __init__(self, OUTPUT_LAYER):\n",
    "        super(HLCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 4 - 4 // 1 + 1 = 4\n",
    "            # nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            # nn.ReLU(),  # 32x4x4\n",
    "            # nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            # nn.ReLU(),  # 64x4x4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            nn.ReLU(),  # 64x4x4\n",
    "            nn.Flatten(),   # 64*4*4 = 1024\n",
    "            nn.Linear(1024, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, OUTPUT_LAYER),  # 512\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(state, network, nA, epsilon, invalid_actions):\n",
    "    # Decide if we are going to be greedy or not\n",
    "    greedy = (random.random() > epsilon)\n",
    "\n",
    "    if greedy:\n",
    "        # Pick best action, if tie, use lowest index\n",
    "        with torch.no_grad():   # Speeds up computation\n",
    "            if (not CNN):\n",
    "                # Change for NN\n",
    "                # output = network(state.view(16,16))\n",
    "                output = network(state)\n",
    "                for invalid in invalid_actions:\n",
    "                    output[invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "            else:\n",
    "                # Change for CNN\n",
    "                output = network(state.view(1,16,4,4))\n",
    "                # output = network(state.view(1,1,4,4))\n",
    "                for invalid in invalid_actions:\n",
    "                    output[0, invalid] = -torch.inf\n",
    "                return output.argmax().item()\n",
    "                # return network(state.view(1,1,4,4)).argmax().item()\n",
    "\n",
    "    else:\n",
    "        # Explore\n",
    "        valid_actions = list(range(nA))\n",
    "        for invalid in invalid_actions:\n",
    "            valid_actions.remove(invalid)\n",
    "        return random.choice(valid_actions)\n",
    "        # return tensor(random.randint(0, nA-1), device=device, dtype=torch.long).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# target_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# policy_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# target_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# policy_net = HLCNN(nA).to(device)\n",
    "# target_net = HLCNN(nA).to(device)\n",
    "# policy_net = DQCNNS(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# target_net = DQCNNS(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "policy_net = DQNM(nS, nA).to(device)\n",
    "target_net = DQNM(nS, nA).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=ALPHA, amsgrad=True)\n",
    "RB = ReplayBuffer(BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = 0\n",
    "# def getEpsilon():\n",
    "#     global steps\n",
    "#     epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-steps/EPSILON_DECAY)\n",
    "#     steps += 1\n",
    "#     return epsilon\n",
    "\n",
    "max_game = 1\n",
    "def getEpsilon(game_duration):\n",
    "    global max_game\n",
    "    epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-game_duration/(max_game))\n",
    "    return epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(title, axis, args, save_string=\"\"):\n",
    "    threshold = 50\n",
    "    n_plots = len(args)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(n_plots, 1, sharex=True)\n",
    "    for argi, arg in enumerate(args):\n",
    "        data = torch.tensor(arg, dtype=torch.float)\n",
    "        ax[argi].set_title(title[argi])\n",
    "        ax[argi].set_ylabel(axis[argi])\n",
    "        ax[argi].step(list(range(len(data))), data)\n",
    "        ax[argi].grid()\n",
    "        ax[argi].minorticks_on()\n",
    "\n",
    "\n",
    "        # Take threshold episode averages and plot them too\n",
    "        if len(arg) >= threshold:\n",
    "            means = data.unfold(0, threshold, 1).mean(1).view(-1)\n",
    "            ax[argi].plot(list(range(threshold-1, len(data))), means.numpy())\n",
    "            \n",
    "    plt.xlabel('Episode')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)\n",
    "    if save_string != \"\":\n",
    "        fig.savefig(\"./figures/\"+save_string+\".png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    minibatch_awk = RB.sample(BATCH_SIZE)\n",
    "    minibatch = SARST(*zip(*minibatch_awk))\n",
    "    \n",
    "    N = len(minibatch.S)\n",
    "    S = torch.cat(minibatch.S).to(device)\n",
    "    # if (True):\n",
    "    #     # S = S.view(N, 1, 4, 4)\n",
    "    #     S = S.view(N, 16, 4, 4)\n",
    "    S = S.view(N, 256)\n",
    "    A = minibatch.A\n",
    "    torch_R = torch.cat(minibatch.R).to(device)\n",
    "    torch_maxQ = torch.zeros(N, 1).to(device)\n",
    "    nonterm_mask = tensor(minibatch.T).to(device)\n",
    "    # term_mask = tensor(list(np.array(minibatch.T)==False))\n",
    "        \n",
    "    Q_SA = policy_net(S).gather(1, torch.reshape(tensor(A).to(device), [N, 1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "        S_prime_masked = torch.cat(S_prime_masked).view(sum(nonterm_mask).item(), -1)  # Adjust this to match the actual dimensions\n",
    "        torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "        # S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "        # torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked).to(device).view(sum(nonterm_mask), 1, 4, 4)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "\n",
    "    y = (torch_maxQ * GAMMA) + torch_R\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(Q_SA, y)\n",
    "\n",
    "    # Clipping the loss between -1 and 1\n",
    "    # loss = torch.mean(torch.maximum(torch.minimum((Q_SA-y)**2, tensor(1)), tensor(-1)))\n",
    "    # torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 100)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = []\n",
    "episodic_loss = []\n",
    "episodic_epsilon = []\n",
    "episdoic_duration = []\n",
    "episdoic_max_tile = []\n",
    "episodic_invalid_moves_made_count = []\n",
    "\n",
    "# use_midgame_board = False\n",
    "# BB = BoardBuffer(100)\n",
    "# BC = BoardCache(50)\n",
    "\n",
    "def DQN_network(episodes):\n",
    "    global max_game\n",
    "    save_tag = \"duration_and_whitespace\"\n",
    "    start_time = time.time()\n",
    "    T = 0\n",
    "    for epi in range(episodes):\n",
    "        \n",
    "        if (not XTRA_IN):\n",
    "            S = env.reset()   \n",
    "        else:\n",
    "            S = np.append(env.reset(), [env.game_duration, env.largest_value])\n",
    "        \n",
    "        if (OHE):\n",
    "            S = encode_state(S).flatten()\n",
    "            S = torch.tensor(S, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_mean_loss = 0\n",
    "        terminated = False\n",
    "        episodic_invalid_moves_made = 0\n",
    "        \n",
    "        mini_duration = 0\n",
    "\n",
    "        invalid_moves = []\n",
    "        while not terminated:\n",
    "            T += 1\n",
    "            mini_duration += 1\n",
    "\n",
    "            epsilon = getEpsilon(env.game_duration)\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, epsilon, invalid_moves)\n",
    "            # Take step\n",
    "            (reward, terminated, updated, invalid_moves, invalid_moves_made) = env.swipe(action_dict[A])\n",
    "            \n",
    "            if (not XTRA_IN):\n",
    "                S_prime = env.get_flat_board()\n",
    "            else:\n",
    "                S_prime = np.append(env.get_flat_board(), [env.game_duration, env.largest_value])\n",
    "            \n",
    "            episodic_invalid_moves_made += invalid_moves_made\n",
    "\n",
    "            if (OHE):\n",
    "                S_prime = encode_state(S_prime).flatten()\n",
    "                S_prime = [0] if terminated else tensor(S_prime, dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "                S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition\n",
    "            # RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "            #         S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "            RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                    S_prime, not terminated)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            # Update the networks networks\n",
    "            if len(RB) > BATCH_SIZE:\n",
    "                episodic_mean_loss += train()\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            if T%10==0:\n",
    "                # Soft update of the target network's weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        # if (mini_duration > 50):\n",
    "        #     tempBC = BC.get_first()\n",
    "        #     BB.push(tempBC[0], tempBC[1])\n",
    "\n",
    "        episodic_epsilon.append(epsilon)\n",
    "        episodic_loss.append(episodic_mean_loss/T)\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episdoic_duration.append(env.game_duration)\n",
    "        episdoic_max_tile.append(max(env.get_flat_board()))\n",
    "        episodic_invalid_moves_made_count.append(episodic_invalid_moves_made)\n",
    "\n",
    "        if epi % 100 == 0:\n",
    "            save_string = \"_policy_weights_episode_\"+str(epi).zfill(4)\n",
    "            torch.save(target_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"target\"+save_string+\".pth\")\n",
    "            torch.save(policy_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"policy\"+save_string+\".pth\")\n",
    "                \n",
    "            # # Soft update of the target network's weights (do at end of episode)\n",
    "            # target_net_state_dict = target_net.state_dict()\n",
    "            # policy_net_state_dict = policy_net.state_dict()\n",
    "            # for key in policy_net_state_dict:\n",
    "            #     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            # target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            \n",
    "        if epi % 1 == 0:\n",
    "            print(episodic_loss[-1])\n",
    "            env.display()\n",
    "            plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Tile Value...\", \"Epsilon Value...\"], \n",
    "                        [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "                        [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_epsilon])\n",
    "        \n",
    "                \n",
    "    \n",
    "    delta_time = time.time()-start_time\n",
    "    plot_multi([\"Reward History\", \"Loss History\", \"Duration\", \"Max Tile History\", \"Epsilon History\"], \n",
    "               [\"Reward\", \"Loss\", \"Duration\", \"Max Tile Value\", \"Epsilon\"], \n",
    "               [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_epsilon], save_string=\"model_\"+save_tag+\"\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save data\n",
    "    # data_file = open(\"./trainged_models/data.json\", 'w+')\n",
    "    # json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "    #                 \"episodic_loss\": episodic_loss, \n",
    "    #                 \"episodic_epsilon\": episodic_epsilon,\n",
    "    #                 \"training_time\": delta_time\n",
    "    #                 }\n",
    "    # json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudde\\AppData\\Local\\Temp\\ipykernel_1380\\1752343122.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  S = torch.tensor(S, dtype=torch.float32, device=device)\n",
      "C:\\Users\\sudde\\AppData\\Local\\Temp\\ipykernel_1380\\1752343122.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  S_prime = [0] if terminated else tensor(S_prime, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "DQN_network(EPISODES_TRAINING)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = open(\"./trainged_models/data_CNN.json\", 'w+')\n",
    "# json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "#                 \"episodic_loss\": episodic_loss, \n",
    "#                 \"episodic_epsilon\": episodic_epsilon,\n",
    "#                 \"training_time\": delta_time\n",
    "#                 }\n",
    "# json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Game' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Game' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "def test(episodes):\n",
    "    episodic_rewards = []\n",
    "    episodic_durations = []\n",
    "\n",
    "    for epi in range(episodes):\n",
    "\n",
    "        \n",
    "        S = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_duration = 0\n",
    "        T = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            episodic_duration += 1\n",
    "            env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, -1)\n",
    "            # Take step\n",
    "            S_prime, reward, terminated, _ = env.step(A)\n",
    "            S = S_prime\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episodic_durations.append(episodic_duration)\n",
    "        # if epi % 10 == 0:\n",
    "        print(epi, \"of\", episodes)\n",
    "        plot_multi([\"Training Rewards...\", \"Training Durations...\"], \n",
    "                    [\"Reward\", \"Episode Duration\"], \n",
    "                    [episodic_rewards, episodic_durations])\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_FUCK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
