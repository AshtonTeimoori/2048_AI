{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "# from torchsummary import summary\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# So we can run off of the GPU for our tensors\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# else:\n",
    "device = \"cpu\"\n",
    "\n",
    "# Live plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nS = env.observation_space.shape[0]\n",
    "nA = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 2**8\n",
    "\n",
    "LAYER1_SIZE = 2**8\n",
    "LAYER2_SIZE = 2**8\n",
    "\n",
    "EPISODES_TRAINING = 2000\n",
    "\n",
    "ALPHA = 1e-4\n",
    "GAMMA = 0.99\n",
    "# TAU = 0.005\n",
    "TAU = 0.01\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_DECAY = 150\n",
    "# EPSILON_DATA = [EPSILON_MAX, EPSILON_MIN, EPSILON_DECAY]\n",
    "\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARST = namedtuple(\"SARST\", [\"S\", \"A\", \"R\", \"S_prime\", \"T\"])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(SARST(*args))\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_LAYER, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_LAYER, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(state, network, nA, epsilon):\n",
    "    # Decide if we are going to be greedy or not\n",
    "    greedy = (random.random() > epsilon)\n",
    "\n",
    "    if greedy:\n",
    "        # Pick best action, if tie, use lowest index\n",
    "        with torch.no_grad():   # Speeds up computation\n",
    "            return network(torch.FloatTensor(state)).argmax().item()\n",
    "\n",
    "    else:\n",
    "        # Explore\n",
    "        return tensor(random.randint(0, nA-1), device=device, dtype=torch.long).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "target_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=ALPHA, amsgrad=True)\n",
    "RB = ReplayBuffer(BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "def getEpsilon():\n",
    "    global steps\n",
    "    epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-steps/EPSILON_DECAY)\n",
    "    steps += 1\n",
    "    return epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(title, axis, args, save_string=\"\"):\n",
    "    n_plots = len(args)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(n_plots, 1, sharex=True)\n",
    "    for argi, arg in enumerate(args):\n",
    "        data = torch.tensor(arg, dtype=torch.float)\n",
    "        # ax[argi].clf()\n",
    "        # ax[argi].clear()\n",
    "        ax[argi].set_title(title[argi])\n",
    "        ax[argi].set_ylabel(axis[argi])\n",
    "        # ax[argi].set_xlabel('Episode')\n",
    "        ax[argi].plot(data)\n",
    "\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(arg) >= 100:\n",
    "            means = data.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            ax[argi].plot(means.numpy())\n",
    "            \n",
    "    plt.xlabel('Episode')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)\n",
    "    if save_string != \"\":\n",
    "        fig.savefig(\"..\\\\P2_Data\\\\model1\\\\\"+save_string+\".png\")\n",
    "\n",
    "        # if is_ipython:\n",
    "        #     display.display(plt.gcf())\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    minibatch_awk = RB.sample(BATCH_SIZE)\n",
    "    minibatch = SARST(*zip(*minibatch_awk))\n",
    "    \n",
    "    N = len(minibatch.S)\n",
    "\n",
    "    S = torch.cat(minibatch.S)\n",
    "    A = minibatch.A\n",
    "    torch_R = torch.cat(minibatch.R)\n",
    "    torch_maxQ = torch.zeros(N, 1)\n",
    "    nonterm_mask = tensor(minibatch.T)\n",
    "    term_mask = tensor(list(np.array(minibatch.T)==False))\n",
    "        \n",
    "    Q_SA = policy_net(S).gather(1, torch.reshape(tensor(A), [N, 1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "        torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "\n",
    "    y = (torch_maxQ * GAMMA) + torch_R\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(Q_SA, y)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = []\n",
    "episodic_loss = []\n",
    "episodic_epsilon = []\n",
    "\n",
    "\n",
    "def DQN_network(episodes):\n",
    "    start_time = time.time()\n",
    "    T = 0\n",
    "    for epi in range(episodes):\n",
    "        \n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_mean_loss = 0\n",
    "        terminated = False\n",
    "        \n",
    "        epsilon = getEpsilon()\n",
    "        \n",
    "        while not terminated:\n",
    "            T += 1\n",
    "            if True and epi%10==0:\n",
    "                env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, epsilon)\n",
    "            # Take step\n",
    "            S_prime, reward, terminated, _, _ = env.step(A)\n",
    "\n",
    "            # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "            S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition\n",
    "            RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                    S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "\n",
    "            S = S_prime\n",
    "\n",
    "            # Update the networks networks\n",
    "            if len(RB) > BATCH_SIZE:\n",
    "                episodic_mean_loss += train()\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            if T%10==0:\n",
    "                # Soft update of the target network's weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        episodic_epsilon.append(epsilon)\n",
    "        episodic_loss.append(episodic_mean_loss/T)\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "\n",
    "        if epi % 10 == 0:\n",
    "            save_string = \"_policy_weights_episode_\"+str(epi).zfill(4)\n",
    "            # torch.save(target_net.state_dict(), \"..\\\\P2_Data\\\\model1\\\\\"+\"target\"+save_string+\".pth\")\n",
    "            # torch.save(policy_net.state_dict(), \"..\\\\P2_Data\\\\model1\\\\\"+\"policy\"+save_string+\".pth\")\n",
    "                \n",
    "            # # Soft update of the target network's weights (do at end of episode)\n",
    "            # target_net_state_dict = target_net.state_dict()\n",
    "            # policy_net_state_dict = policy_net.state_dict()\n",
    "            # for key in policy_net_state_dict:\n",
    "            #     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            # target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            print(epsilon)\n",
    "            plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Epsilon Value...\"], \n",
    "                       [\"Reward\", \"Mean Episode Loss\", \"Epsilon\"], \n",
    "                       [episodic_rewards, episodic_loss, episodic_epsilon])\n",
    "        \n",
    "                \n",
    "    \n",
    "    delta_time = time.time()-start_time\n",
    "    plot_multi([\"Reward History\", \"Loss History\", \"Epsilon History\"], \n",
    "               [\"Reward\", \"Loss\", \"Epsilon\"], \n",
    "               [episodic_rewards, episodic_loss, episodic_epsilon], save_string=\"model1\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save data\n",
    "    data_file = open(\"..\\\\P2_Data\\\\model1\\\\data.json\", 'w+')\n",
    "    json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "                    \"episodic_loss\": episodic_loss, \n",
    "                    \"episodic_epsilon\": episodic_epsilon,\n",
    "                    \"training_time\": delta_time\n",
    "                    }\n",
    "    json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDQN_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES_TRAINING\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m, in \u001b[0;36mDQN_network\u001b[1;34m(episodes)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Update the networks networks\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(RB) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m---> 41\u001b[0m     episodic_mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m torch_maxQ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m nonterm_mask \u001b[38;5;241m=\u001b[39m tensor(minibatch\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 12\u001b[0m term_mask \u001b[38;5;241m=\u001b[39m tensor(\u001b[38;5;28mlist\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     14\u001b[0m Q_SA \u001b[38;5;241m=\u001b[39m policy_net(S)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, torch\u001b[38;5;241m.\u001b[39mreshape(tensor(A), [N, \u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DQN_network(EPISODES_TRAINING)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "def test(episodes):\n",
    "    episodic_rewards = []\n",
    "    episodic_durations = []\n",
    "\n",
    "    for epi in range(episodes):\n",
    "\n",
    "        \n",
    "        S = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_duration = 0\n",
    "        T = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            episodic_duration += 1\n",
    "            env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, -1)\n",
    "            # Take step\n",
    "            S_prime, reward, terminated, _ = env.step(A)\n",
    "            S = S_prime\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episodic_durations.append(episodic_duration)\n",
    "        # if epi % 10 == 0:\n",
    "        print(epi, \"of\", episodes)\n",
    "        plot_multi([\"Training Rewards...\", \"Training Durations...\"], \n",
    "                    [\"Reward\", \"Episode Duration\"], \n",
    "                    [episodic_rewards, episodic_durations])\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_FUCK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
