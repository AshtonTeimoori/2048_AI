{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "# from torchsummary import summary\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import time\n",
    "import json\n",
    "from src.Runner2048 import Game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can run off of the GPU for our tensors\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Live plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Game(seed=1, board_size=4, reward_type='duration_and_largest')\n",
    "env = Game(seed=1, board_size=4, reward_type='end_of_game_reward')\n",
    "action_dict = {0:'U', 1:'R', 2:'D', 3:'L'}\n",
    "\n",
    "# env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nS = 16\n",
    "nA = 4\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 2**7\n",
    "# BATCH_SIZE = 2**5\n",
    "\n",
    "LAYER1_SIZE = 2**9\n",
    "LAYER2_SIZE = 2**9\n",
    "\n",
    "# EPISODES_TRAINING = 1000\n",
    "# EPISODES_TRAINING = 2000\n",
    "EPISODES_TRAINING = 50000\n",
    "\n",
    "ALPHA = 1e-1\n",
    "GAMMA = 0.99\n",
    "# TAU = 0.005\n",
    "TAU = 0.01\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 250\n",
    "# EPSILON_DATA = [EPSILON_MAX, EPSILON_MIN, EPSILON_DECAY]\n",
    "\n",
    "# BUFFER_SIZE = 10000\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARST = namedtuple(\"SARST\", [\"S\", \"A\", \"R\", \"S_prime\", \"T\"])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(SARST(*args))\n",
    "        # Rotate the board and add it to the buffer\n",
    "        if (True):\n",
    "            # 90 degrees CCW\n",
    "            S_new = tensor([np.rot90(args[0].cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (args[1]-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(args[3].cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 180 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # 270 degrees CCW\n",
    "            S_new = tensor([np.rot90(S_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            A_new = (A_new-1)%4\n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.rot90(S_prime_new.cpu().view(4,4)).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "            # Flip Virt\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            if (args[1]%2 == 0):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=0).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "            \n",
    "            # Flip Horz\n",
    "            S_new = tensor([np.flip(args[0].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            if (args[1]%2 == 1):\n",
    "                A_new = (args[1]-2)%4\n",
    "            else:\n",
    "                A_new = args[1]\n",
    "            \n",
    "            if (args[3] == [0]):\n",
    "                S_prime_new = args[3]\n",
    "            else:\n",
    "                S_prime_new = tensor([np.flip(args[3].cpu().view(4,4).numpy(), axis=1).flatten()]).to(device)\n",
    "            self.buffer.append(SARST(S_new, A_new, args[2], S_prime_new, args[4]))\n",
    "\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDBUFF = namedtuple(\"BOARDBUFF\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDBUFF(*args))\n",
    "\n",
    "    # def sample(self, sample_size):\n",
    "    #     return random.sample(self.buffer, sample_size)\n",
    "\n",
    "    def pop_sample(self):\n",
    "        pop_index = random.randint(0, len(self.buffer)-1)\n",
    "        return_board = self.buffer[pop_index]\n",
    "        del self.buffer[pop_index]\n",
    "        return return_board\n",
    "    \n",
    "    # def remove(self, occurence):\n",
    "    #     self.buffer.remove(occurence)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "BOARDCACHE = namedtuple(\"BOARDCACHE\", [\"S\", \"D\"]) # Board state, Game duration\n",
    "\n",
    "class BoardCache(object):\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(BOARDCACHE(*args))\n",
    "\n",
    "    def get_first(self):\n",
    "        return self.buffer.popleft()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_LAYER, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_LAYER, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "class DQCNN(nn.Module):\n",
    "    def __init__(self, KERNEL_SIZE, LAYER1_SIZE, LAYER2_SIZE, OUTPUT_LAYER):\n",
    "        super(DQCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 2 - 3 // 1 + 1 = 4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 1x4x4  -> 16x4x4\n",
    "            nn.ReLU(),  # 16x4x4\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=KERNEL_SIZE, stride=1, padding=1),  # 16x4x4 -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Flatten(),   # 32*4*4 = 512\n",
    "            nn.Linear(512, LAYER1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER1_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, LAYER2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(LAYER2_SIZE, OUTPUT_LAYER),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "class HLCNN(nn.Module): # Human Level Control NN (Paper)\n",
    "    def __init__(self, OUTPUT_LAYER):\n",
    "        super(HLCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(   #(H + self.padding*2 - self.kernel_size) // self.stride + 1 -> 4 + 4 - 4 // 1 + 1 = 4\n",
    "            # nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            # nn.ReLU(),  # 32x4x4\n",
    "            # nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            # nn.ReLU(),  # 64x4x4\n",
    "            nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=4, stride=1, padding='same'),  # 1x4x4  -> 32x4x4\n",
    "            nn.ReLU(),  # 32x4x4\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same'),  # 32x4x4 -> 64x4x4\n",
    "            nn.ReLU(),  # 64x4x4\n",
    "            nn.Flatten(),   # 64*4*4 = 1024\n",
    "            nn.Linear(1024, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, 2**9),  # 2^9 = 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2**9, OUTPUT_LAYER),  # 512\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(state, network, nA, epsilon):\n",
    "    # Decide if we are going to be greedy or not\n",
    "    greedy = (random.random() > epsilon)\n",
    "\n",
    "    if greedy:\n",
    "        # Pick best action, if tie, use lowest index\n",
    "        with torch.no_grad():   # Speeds up computation\n",
    "            # return network(torch.FloatTensor(state)).argmax().item()\n",
    "            return network(state.view(1,1,4,4)).argmax().item()\n",
    "\n",
    "    else:\n",
    "        # Explore\n",
    "        return tensor(random.randint(0, nA-1), device=device, dtype=torch.long).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# target_net = DQN(nS, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# policy_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "# target_net = DQCNN(3, LAYER1_SIZE, LAYER2_SIZE, nA).to(device)\n",
    "policy_net = HLCNN(nA).to(device)\n",
    "target_net = HLCNN(nA).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=ALPHA, amsgrad=True)\n",
    "RB = ReplayBuffer(BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "def getEpsilon():\n",
    "    global steps\n",
    "    epsilon = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN)*exp(-steps/EPSILON_DECAY)\n",
    "    steps += 1\n",
    "    return epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(title, axis, args, save_string=\"\"):\n",
    "    n_plots = len(args)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(n_plots, 1, sharex=True)\n",
    "    for argi, arg in enumerate(args):\n",
    "        data = torch.tensor(arg, dtype=torch.float)\n",
    "        # ax[argi].clf()\n",
    "        # ax[argi].clear()\n",
    "        ax[argi].set_title(title[argi])\n",
    "        ax[argi].set_ylabel(axis[argi])\n",
    "        # ax[argi].set_xlabel('Episode')\n",
    "        ax[argi].plot(data)\n",
    "\n",
    "        # Take 50 episode averages and plot them too\n",
    "        if len(arg) >= 50:\n",
    "            means = data.unfold(0, 50, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(49), means))\n",
    "            ax[argi].plot(means.numpy())\n",
    "            \n",
    "    plt.xlabel('Episode')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)\n",
    "    if save_string != \"\":\n",
    "        fig.savefig(\"./figures/\"+save_string+\".png\")\n",
    "\n",
    "        # if is_ipython:\n",
    "        #     display.display(plt.gcf())\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    minibatch_awk = RB.sample(BATCH_SIZE)\n",
    "    minibatch = SARST(*zip(*minibatch_awk))\n",
    "    \n",
    "    N = len(minibatch.S)\n",
    "\n",
    "    S = torch.cat(minibatch.S).to(device)\n",
    "    S = S.view(N, 1, 4, 4)\n",
    "    A = minibatch.A\n",
    "    torch_R = torch.cat(minibatch.R).to(device)\n",
    "    torch_maxQ = torch.zeros(N, 1).to(device)\n",
    "    nonterm_mask = tensor(minibatch.T).to(device)\n",
    "    # term_mask = tensor(list(np.array(minibatch.T)==False))\n",
    "        \n",
    "    Q_SA = policy_net(S).gather(1, torch.reshape(tensor(A).to(device), [N, 1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "        # torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "        S_prime_masked =  list(compress(minibatch.S_prime, minibatch.T))\n",
    "        torch_maxQ[nonterm_mask] = torch.reshape(target_net(torch.cat(S_prime_masked).to(device).view(sum(nonterm_mask), 1, 4, 4)).max(1)[0], [sum(nonterm_mask).item(), 1])\n",
    "\n",
    "    y = (torch_maxQ * GAMMA) + torch_R\n",
    "\n",
    "    # Compute Huber loss\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    # loss = criterion(Q_SA, y)\n",
    "\n",
    "    # Clipping the loss between -1 and 1\n",
    "    loss = torch.mean(torch.maximum(torch.minimum((Q_SA-y)**2, tensor(1)), tensor(-1)))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = []\n",
    "episodic_loss = []\n",
    "episodic_epsilon = []\n",
    "episdoic_duration = []\n",
    "episdoic_max_tile = []\n",
    "episodic_revision_count = []\n",
    "\n",
    "# use_midgame_board = False\n",
    "# BB = BoardBuffer(100)\n",
    "# BC = BoardCache(50)\n",
    "\n",
    "def DQN_network(episodes):\n",
    "    save_tag = \"end_of_game_reward_good_moves_only_log2\"\n",
    "    start_time = time.time()\n",
    "    T = 0\n",
    "    for epi in range(episodes):\n",
    "        \n",
    "        # BC.clear()\n",
    "        # if (use_midgame_board and (len(BB) > 100) and (0.1 > random.random())): # Randomly select from the board buffer\n",
    "        #     loadBB = BB.pop_sample()\n",
    "        #     S = env.load_board(loadBB.S, loadBB.D)\n",
    "        #     S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # else:\n",
    "        S = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "            # BC.push(S, 0)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_mean_loss = 0\n",
    "        terminated = False\n",
    "        episodic_revision = 0\n",
    "        \n",
    "        epsilon = getEpsilon()\n",
    "        \n",
    "        mini_duration = 0\n",
    "        while not terminated:\n",
    "            T += 1\n",
    "            mini_duration += 1\n",
    "            # if True and epi%10==0:\n",
    "            #     env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, epsilon)\n",
    "            # Take step\n",
    "            reward, terminated, updated, A, revision_count = env.swipe(action_dict[A])\n",
    "            S_prime = env.get_flat_board()\n",
    "            episodic_revision += revision_count\n",
    "            \n",
    "            # S_prime = None if terminated else tensor(torch.FloatTensor(S_prime).to(device), requires_grad=True)\n",
    "            S_prime = [0] if terminated else tensor([S_prime], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition\n",
    "            # RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "            #         S_prime, tensor(not terminated, device=device, dtype=torch.bool))\n",
    "            RB.push(S, A, tensor([[reward]], dtype=torch.float32, device=device), \n",
    "                    S_prime, not terminated)\n",
    "\n",
    "            S = S_prime\n",
    "            # if (terminated):\n",
    "            #     BC.push(S, env.game_duration) # Just filler really \n",
    "            # else:\n",
    "            #     BC.push(S.view(4,4).cpu().numpy(), env.game_duration)\n",
    "            \n",
    "            # Update the networks networks\n",
    "            if len(RB) > BATCH_SIZE:\n",
    "                episodic_mean_loss += train()\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            if T%10==0:\n",
    "                # Soft update of the target network's weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        # if (mini_duration > 50):\n",
    "        #     tempBC = BC.get_first()\n",
    "        #     BB.push(tempBC[0], tempBC[1])\n",
    "\n",
    "        episodic_epsilon.append(epsilon)\n",
    "        episodic_loss.append(episodic_mean_loss/T)\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episdoic_duration.append(env.game_duration)\n",
    "        episdoic_max_tile.append(max(env.get_flat_board()))\n",
    "        episodic_revision_count.append(episodic_revision)\n",
    "\n",
    "        if epi % 100 == 0:\n",
    "            save_string = \"_policy_weights_episode_\"+str(epi).zfill(4)\n",
    "            torch.save(target_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"target\"+save_string+\".pth\")\n",
    "            torch.save(policy_net.state_dict(), \"./trained_models/\"+save_tag+\"_\"+\"policy\"+save_string+\".pth\")\n",
    "                \n",
    "            # # Soft update of the target network's weights (do at end of episode)\n",
    "            # target_net_state_dict = target_net.state_dict()\n",
    "            # policy_net_state_dict = policy_net.state_dict()\n",
    "            # for key in policy_net_state_dict:\n",
    "            #     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            # target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            # print()\n",
    "            \n",
    "        if epi % 50 == 0:\n",
    "            print(epsilon)\n",
    "            env.display()\n",
    "            plot_multi([\"Training Rewards...\", \"Training Loss...\", \"Duration...\", \"Max Episodic Value...\", \"Invalid Moves...\", \"Epsilon Value...\"], \n",
    "                       [\"Reward\", \"Mean Episode Loss\", \"Duration\", \"Max Tile Value\", \"Invalid Moves\", \"Epsilon\"], \n",
    "                       [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_revision_count, episodic_epsilon])\n",
    "        \n",
    "                \n",
    "    \n",
    "    delta_time = time.time()-start_time\n",
    "    plot_multi([\"Reward History\", \"Loss History\", \"Duration\", \"Max Episodic Value\", \"Invalid Moves\", \"Epsilon History\"], \n",
    "               [\"Reward\", \"Loss\", \"Duration\", \"Max Tile Value\", \"Invalid Moves\", \"Epsilon\"], \n",
    "               [episodic_rewards, episodic_loss, episdoic_duration, episdoic_max_tile, episodic_revision_count, episodic_epsilon], save_string=\"model_\"+save_tag+\"\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save data\n",
    "    # data_file = open(\"./trainged_models/data.json\", 'w+')\n",
    "    # json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "    #                 \"episodic_loss\": episodic_loss, \n",
    "    #                 \"episodic_epsilon\": episodic_epsilon,\n",
    "    #                 \"training_time\": delta_time\n",
    "    #                 }\n",
    "    # json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_network(EPISODES_TRAINING)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = open(\"./trainged_models/data_CNN.json\", 'w+')\n",
    "# json_data = {\"episodic_rewards\": episodic_rewards, \n",
    "#                 \"episodic_loss\": episodic_loss, \n",
    "#                 \"episodic_epsilon\": episodic_epsilon,\n",
    "#                 \"training_time\": delta_time\n",
    "#                 }\n",
    "# json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "def test(episodes):\n",
    "    episodic_rewards = []\n",
    "    episodic_durations = []\n",
    "\n",
    "    for epi in range(episodes):\n",
    "\n",
    "        \n",
    "        S = env.reset()\n",
    "        S = torch.tensor([S], dtype=torch.float32, device=device)\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_duration = 0\n",
    "        T = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            episodic_duration += 1\n",
    "            env.render()\n",
    "\n",
    "            # Choose action\n",
    "            A = epsilonGreedy(S, policy_net, nA, -1)\n",
    "            # Take step\n",
    "            S_prime, reward, terminated, _ = env.step(A)\n",
    "            S = S_prime\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "        episodic_durations.append(episodic_duration)\n",
    "        # if epi % 10 == 0:\n",
    "        print(epi, \"of\", episodes)\n",
    "        plot_multi([\"Training Rewards...\", \"Training Durations...\"], \n",
    "                    [\"Reward\", \"Episode Duration\"], \n",
    "                    [episodic_rewards, episodic_durations])\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_FUCK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
